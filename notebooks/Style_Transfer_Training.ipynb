{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8069a2",
   "metadata": {},
   "source": [
    "# Style Transfer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b836130",
   "metadata": {},
   "source": [
    "This notebook will be used to train the style transfer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9e7b43",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1007e",
   "metadata": {},
   "source": [
    "### 1.1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d91edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4489c81",
   "metadata": {},
   "source": [
    "### 1.2. Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14a3d8",
   "metadata": {},
   "source": [
    "Make sure your dataset is accessible via a shareable link from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262eeb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "\n",
    "# Replace with the actual file ID of your dataset from Google Drive\n",
    "file_id = '1acx_7IdpUg3OBXSRfnNyyjiZlIiQCE8M' \n",
    "output = 'dataset.zip'\n",
    "\n",
    "gdown.download(id=file_id, output=output, quiet=False)\n",
    "\n",
    "# Unzip the dataset if it's a zip file\n",
    "!unzip -q dataset.zip -d /content/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d58e02",
   "metadata": {},
   "source": [
    "## 2. Prepare for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54941145",
   "metadata": {},
   "source": [
    "### 2.1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/diffusers.git transformers accelerate peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f1c55",
   "metadata": {},
   "source": [
    "### 2.2. Define Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model from Hugging Face\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# Directory with your training images\n",
    "INSTANCE_DIR = \"/content/dataset\"\n",
    "\n",
    "# Directory to save the trained model\n",
    "OUTPUT_DIR = \"/content/models/style-transfer-model\"\n",
    "\n",
    "# The unique prompt that will trigger your style\n",
    "# \"sks\" is a placeholder for a unique token, which is a common practice.\n",
    "INSTANCE_PROMPT = \"a drawing in sks style\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a7757",
   "metadata": {},
   "source": [
    "### 2.3. Preprocess Images and Create Captions\n",
    "\n",
    "This cell will prepare your dataset for training. It does two things:\n",
    "1. Resizes all your images to 512x512 pixels, which is the standard for Stable Diffusion 1.5.\n",
    "2. Creates a text file for each image with your unique prompt (`INSTANCE_PROMPT`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd02650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "image_files = [f for f in os.listdir(INSTANCE_DIR) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "for filename in image_files:\n",
    "    # Create caption file\n",
    "    caption_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "    with open(os.path.join(INSTANCE_DIR, caption_filename), \"w\") as f:\n",
    "        f.write(INSTANCE_PROMPT)\n",
    "\n",
    "    # Resize image\n",
    "    image_path = os.path.join(INSTANCE_DIR, filename)\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # Ensure image is in RGB mode\n",
    "            img = img.convert(\"RGB\")\n",
    "            img = img.resize((512, 512), Image.LANCZOS)\n",
    "            img.save(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process {filename}: {e}\")\n",
    "\n",
    "print(f\"Processed {len(image_files)} images and created caption files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ad08a",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f1269",
   "metadata": {},
   "source": [
    "### 3.1. Download the Training Script\n",
    "\n",
    "We'll use the official LoRA training script from the Hugging Face `diffusers` library. This script is well-maintained and contains all the logic for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef091cec",
   "metadata": {},
   "source": [
    "### 3.2. Launch the Training\n",
    "\n",
    "Now we'll launch the training process using `accelerate`. This command passes all our defined parameters to the training script.\n",
    "\n",
    "**Important:** This step will take a while and requires a GPU. Make sure your Colab runtime is set to **GPU** (Runtime -> Change runtime type -> Hardware accelerator -> GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some recommended training parameters. You can adjust them later.\n",
    "TRAIN_BATCH_SIZE=1\n",
    "LEARNING_RATE=1e-4\n",
    "LR_SCHEDULER=\"constant\"\n",
    "LR_WARMUP_STEPS=0\n",
    "MAX_TRAIN_STEPS=1500 # More steps can lead to a stronger style, but also overfitting. 1500 is a good start.\n",
    "SEED=42\n",
    "\n",
    "# Construct the training command as a single block of shell commands\n",
    "training_command = f\"\"\"accelerate launch train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"{MODEL_NAME}\" \\\n",
    "  --instance_data_dir=\"{INSTANCE_DIR}\" \\\n",
    "  --output_dir=\"{OUTPUT_DIR}\" \\\n",
    "  --instance_prompt=\"{INSTANCE_PROMPT}\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size={TRAIN_BATCH_SIZE} \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --checkpointing_steps=500 \\\n",
    "  --learning_rate={LEARNING_RATE} \\\n",
    "  --lr_scheduler=\"{LR_SCHEDULER}\" \\\n",
    "  --lr_warmup_steps={LR_WARMUP_STEPS} \\\n",
    "  --max_train_steps={MAX_TRAIN_STEPS} \\\n",
    "  --seed={SEED}\"\"\"\n",
    "\n",
    "# Run the training\n",
    "!{training_command}\n",
    "\n",
    "print(f\"\\n\\nTraining finished! The trained LoRA model is saved in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72011ccd",
   "metadata": {},
   "source": [
    "## 4. Inference (Generate Images)\n",
    "\n",
    "Congratulations, the training is complete! Now we can use our trained LoRA model to generate images in your unique style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88189975",
   "metadata": {},
   "source": [
    "### 4.1. Load the Model and LoRA Weights\n",
    "\n",
    "This cell loads the original Stable Diffusion model and then applies your trained LoRA weights on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# The base model ID\n",
    "model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# The path to your trained LoRA model directory\n",
    "lora_path = \"/content/models/style-transfer-model\"\n",
    "\n",
    "# Load the pipeline and move it to the GPU\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# Load the LoRA weights\n",
    "pipe.load_lora_weights(lora_path)\n",
    "\n",
    "print(\"Model and LoRA weights loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255f07e",
   "metadata": {},
   "source": [
    "### 4.2. Generate an Image\n",
    "\n",
    "Now, let's generate an image. You can change the prompt to describe the person or scene you want to illustrate. **Crucially, you must include the trigger phrase `\"a drawing in sks style\"`** for your style to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# The prompt for the image generation\n",
    "# Change the beginning of the prompt to describe your subject.\n",
    "# KEEP \"a drawing in sks style\" at the end.\n",
    "prompt = \"photo of a man with glasses and a beard, a drawing in sks style\"\n",
    "\n",
    "# Parameters for generation. You can experiment with these.\n",
    "num_inference_steps = 30  # More steps can improve quality, but take longer.\n",
    "guidance_scale = 7.5  # How much the prompt guides the image generation.\n",
    "generator = torch.Generator(\"cuda\").manual_seed(42) # for reproducible results\n",
    "\n",
    "# Generate the image\n",
    "with torch.no_grad():\n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "\n",
    "# Display the image\n",
    "print(\"Generated Image:\")\n",
    "display(image)\n",
    "\n",
    "# You can also save the image\n",
    "# image.save(\"/content/generated_image.png\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
