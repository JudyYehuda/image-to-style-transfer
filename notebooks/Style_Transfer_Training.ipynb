{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8069a2",
   "metadata": {},
   "source": [
    "# Style Transfer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b836130",
   "metadata": {},
   "source": [
    "This notebook will be used to train the style transfer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9e7b43",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1007e",
   "metadata": {},
   "source": [
    "### 1.1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d91edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4489c81",
   "metadata": {},
   "source": [
    "### 1.2. Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14a3d8",
   "metadata": {},
   "source": [
    "Make sure your dataset is accessible via a shareable link from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262eeb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "\n",
    "# Replace with the actual file ID of your dataset from Google Drive\n",
    "file_id = '1acx_7IdpUg3OBXSRfnNyyjiZlIiQCE8M' \n",
    "output = 'dataset.zip'\n",
    "\n",
    "gdown.download(id=file_id, output=output, quiet=False)\n",
    "\n",
    "# Unzip the dataset if it's a zip file\n",
    "!unzip -q dataset.zip -d /content/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac7962",
   "metadata": {},
   "source": [
    "### 1.3. Fix Dataset Directory Structure (If Needed)\n",
    "\n",
    "This cell checks if the unzipped file created an extra parent folder. If so, it moves all images to the correct location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6293a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "dataset_path = '/content/dataset'\n",
    "unzipped_contents = os.listdir(dataset_path)\n",
    "\n",
    "# Check if there is a single subdirectory inside the dataset folder\n",
    "if len(unzipped_contents) == 1 and os.path.isdir(os.path.join(dataset_path, unzipped_contents[0])):\n",
    "    sub_dir = os.path.join(dataset_path, unzipped_contents[0])\n",
    "    print(f\"Detected single subdirectory: {sub_dir}. Moving its contents up.\")\n",
    "\n",
    "    # Move all files from the subdirectory to the parent\n",
    "    for filename in os.listdir(sub_dir):\n",
    "        source_path = os.path.join(sub_dir, filename)\n",
    "        destination_path = os.path.join(dataset_path, filename)\n",
    "        shutil.move(source_path, destination_path)\n",
    "\n",
    "    # Remove the now-empty subdirectory\n",
    "    os.rmdir(sub_dir)\n",
    "    print(f\"Directory structure fixed. Files are now in {dataset_path}\")\n",
    "else:\n",
    "    print(f\"Dataset structure appears correct. No changes made.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d58e02",
   "metadata": {},
   "source": [
    "## 2. Prepare for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54941145",
   "metadata": {},
   "source": [
    "### 2.1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/diffusers.git transformers accelerate peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f1c55",
   "metadata": {},
   "source": [
    "### 2.2. Define Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model from Hugging Face\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# Directory with your training images\n",
    "INSTANCE_DIR = \"/content/dataset\"\n",
    "\n",
    "# Directory to save the trained model\n",
    "OUTPUT_DIR = \"/content/models/style-transfer-model\"\n",
    "\n",
    "# The unique prompt that will trigger your style\n",
    "# \"sks\" is a placeholder for a unique token, which is a common practice.\n",
    "INSTANCE_PROMPT = \"a drawing in sks style\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a7757",
   "metadata": {},
   "source": [
    "### 2.3. Preprocess Images and Create Captions\n",
    "\n",
    "This cell will prepare your dataset for training. It does two things:\n",
    "1. Resizes all your images to 512x512 pixels, which is the standard for Stable Diffusion 1.5.\n",
    "2. Creates a text file for each image with your unique prompt (`INSTANCE_PROMPT`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd02650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "image_files = [f for f in os.listdir(INSTANCE_DIR) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "for filename in image_files:\n",
    "    # Create caption file\n",
    "    caption_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "    with open(os.path.join(INSTANCE_DIR, caption_filename), \"w\") as f:\n",
    "        f.write(INSTANCE_PROMPT)\n",
    "\n",
    "    # Resize image\n",
    "    image_path = os.path.join(INSTANCE_DIR, filename)\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # Ensure image is in RGB mode\n",
    "            img = img.convert(\"RGB\")\n",
    "            img = img.resize((512, 512), Image.LANCZOS)\n",
    "            img.save(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process {filename}: {e}\")\n",
    "\n",
    "print(f\"Processed {len(image_files)} images and created caption files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ad08a",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f1269",
   "metadata": {},
   "source": [
    "### 3.1. Download the Training Script\n",
    "\n",
    "We'll use the official LoRA training script from the Hugging Face `diffusers` library. This script is well-maintained and contains all the logic for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef091cec",
   "metadata": {},
   "source": [
    "### 3.2. Launch the Training\n",
    "\n",
    "Now we'll launch the training process using `accelerate`. This command passes all our defined parameters to the training script.\n",
    "\n",
    "**Important:** This step will take a while and requires a GPU. Make sure your Colab runtime is set to **GPU** (Runtime -> Change runtime type -> Hardware accelerator -> GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some recommended training parameters. You can adjust them later.\n",
    "TRAIN_BATCH_SIZE=1\n",
    "LEARNING_RATE=1e-4\n",
    "LR_SCHEDULER=\"constant\"\n",
    "LR_WARMUP_STEPS=0\n",
    "MAX_TRAIN_STEPS=1500 # More steps can lead to a stronger style, but also overfitting. 1500 is a good start.\n",
    "SEED=42\n",
    "\n",
    "# Construct the training command as a single block of shell commands\n",
    "training_command = f\"\"\"accelerate launch train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"{MODEL_NAME}\" \\\n",
    "  --instance_data_dir=\"{INSTANCE_DIR}\" \\\n",
    "  --output_dir=\"{OUTPUT_DIR}\" \\\n",
    "  --instance_prompt=\"{INSTANCE_PROMPT}\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size={TRAIN_BATCH_SIZE} \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --checkpointing_steps=500 \\\n",
    "  --learning_rate={LEARNING_RATE} \\\n",
    "  --lr_scheduler=\"{LR_SCHEDULER}\" \\\n",
    "  --lr_warmup_steps={LR_WARMUP_STEPS} \\\n",
    "  --max_train_steps={MAX_TRAIN_STEPS} \\\n",
    "  --seed={SEED}\"\"\"\n",
    "\n",
    "# Run the training\n",
    "!{training_command}\n",
    "\n",
    "print(f\"\\n\\nTraining finished! The trained LoRA model is saved in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb3ed4",
   "metadata": {},
   "source": [
    "### 3.3. Save Trained Model to Google Drive\n",
    "\n",
    "This is a crucial step. To avoid re-training every time, we save the final trained LoRA model to a permanent location in your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount drive to access it\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the source and destination paths\n",
    "source_model_dir = \"/content/models/style-transfer-model\"\n",
    "destination_dir = \"/content/drive/MyDrive/MyTrainedModels\"\n",
    "final_model_path = os.path.join(destination_dir, \"MyStyleLoRA\")\n",
    "\n",
    "# Create the destination directory in Google Drive if it doesn't exist\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "\n",
    "# Copy the trained model files to Google Drive\n",
    "!cp -r {source_model_dir}/* {final_model_path}/\n",
    "\n",
    "print(f\"Model successfully saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72011ccd",
   "metadata": {},
   "source": [
    "## 4. Inference (Generate Images)\n",
    "\n",
    "Congratulations, the training is complete! Now we can use our trained LoRA model to generate images in your unique style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88189975",
   "metadata": {},
   "source": [
    "### 4.1. Load the Model and LoRA Weights\n",
    "\n",
    "This cell loads the original Stable Diffusion model and then applies your trained LoRA weights on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# The base model ID\n",
    "model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# The path to your trained LoRA model directory\n",
    "lora_path = \"/content/models/style-transfer-model\"\n",
    "\n",
    "# Load the pipeline and move it to the GPU\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# Load the LoRA weights\n",
    "pipe.load_lora_weights(lora_path)\n",
    "\n",
    "print(\"Model and LoRA weights loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255f07e",
   "metadata": {},
   "source": [
    "### 4.2. Generate an Image\n",
    "\n",
    "Now, let's generate an image. You can change the prompt to describe the person or scene you want to illustrate. **Crucially, you must include the trigger phrase `\"a drawing in sks style\"`** for your style to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# The prompt for the image generation\n",
    "# Change the beginning of the prompt to describe your subject.\n",
    "# KEEP \"a drawing in sks style\" at the end.\n",
    "prompt = \"photo of a man with glasses and a beard, a drawing in sks style\"\n",
    "\n",
    "# Parameters for generation. You can experiment with these.\n",
    "num_inference_steps = 30  # More steps can improve quality, but take longer.\n",
    "guidance_scale = 7.5  # How much the prompt guides the image generation.\n",
    "generator = torch.Generator(\"cuda\").manual_seed(42) # for reproducible results\n",
    "\n",
    "# Generate the image\n",
    "with torch.no_grad():\n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "\n",
    "# Display the image\n",
    "print(\"Generated Image:\")\n",
    "display(image)\n",
    "\n",
    "# You can also save the image\n",
    "# image.save(\"/content/generated_image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7eb505",
   "metadata": {},
   "source": [
    "## 5. Image-to-Image Transformation (The Real Goal)\n",
    "\n",
    "This section allows you to upload your own photo and transform it into an illustration using your trained style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764d01c",
   "metadata": {},
   "source": [
    "### 5.1. Upload Your Source Image\n",
    "\n",
    "Run this cell and select the photo you want to transform from your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09077f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    source_image_path = list(uploaded.keys())[0]\n",
    "    print(f\"Image \\\"{source_image_path}\\\" uploaded successfully.\")\n",
    "else:\n",
    "    source_image_path = None\n",
    "    print(\"No image uploaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0dcf6",
   "metadata": {},
   "source": [
    "### 5.2. Run the Image-to-Image Pipeline\n",
    "\n",
    "This cell loads a new pipeline (`StableDiffusionImg2ImgPipeline`), applies your style, and transforms the uploaded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "\n",
    "if source_image_path:\n",
    "    # Load the Img2Img pipeline\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n",
    "    pipe.load_lora_weights(lora_path)\n",
    "\n",
    "    # Load and process the source image\n",
    "    source_image = Image.open(source_image_path).convert(\"RGB\")\n",
    "    source_image = source_image.resize((512, 512)) # Resize to match model input\n",
    "\n",
    "    # Define the prompt and parameters\n",
    "    # Remember to include the trigger phrase!\n",
    "    prompt = \"a drawing in sks style, high quality illustration\"\n",
    "    strength = 0.75  # 0.0: keeps original, 1.0: very creative. 0.7-0.8 is a good start.\n",
    "    guidance_scale = 7.5\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(42) # For reproducible results\n",
    "\n",
    "    # Generate the new image\n",
    "    with torch.no_grad():\n",
    "        transformed_image = pipe(\n",
    "            prompt=prompt,\n",
    "            image=source_image,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator\n",
    "        ).images[0]\n",
    "\n",
    "    # Display the result\n",
    "    print(\"Original Image:\")\n",
    "    display(source_image)\n",
    "    print(\"\\nTransformed Image:\")\n",
    "    display(transformed_image)\n",
    "    \n",
    "    # transformed_image.save(\"/content/transformed_image.png\")\n",
    "else:\n",
    "    print(\"Please upload an image in the previous cell first.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
